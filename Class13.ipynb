{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgdFtBTXcqywA+L/6zVC+C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qKWaEkeH_duQ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from math import log2\n","from collections import Counter\n","\n","class J48Node:\n","    \"\"\"Class that represents a decision node or leaf in the J48 tree\"\"\"\n","\n","    def __init__(self, feature=None, threshold=None, children=None, value=None, split_value=None):\n","        self.feature = feature        # Feature to split on\n","        self.threshold = threshold    # Threshold for numerical features\n","        self.children = children or {} # Dictionary of child nodes\n","        self.value = value           # Value if leaf node (class label)\n","        self.split_value = split_value # For handling missing values\n","\n","    def is_leaf(self):\n","        return self.value is not None\n","\n","\n","class J48Classifier:\n","    \"\"\"J48 Decision Tree Classifier (similar to C4.5)\"\"\"\n","\n","    def __init__(self, max_depth=10, min_samples_split=2, confidence_factor=0.25, num_folds=3):\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.confidence_factor = confidence_factor  # For pruning\n","        self.num_folds = num_folds                # For reduced-error pruning\n","        self.root = None\n","\n","    def fit(self, X, y):\n","        \"\"\"Build the J48 decision tree with pruning\"\"\"\n","        if isinstance(X, pd.DataFrame):\n","            X = X.values\n","        if isinstance(y, pd.Series):\n","            y = y.values\n","\n","        self.n_classes = len(np.unique(y))\n","        self.n_features = X.shape[1]\n","\n","        # First grow the tree\n","        self.root = self._grow_tree(X, y)\n","\n","        # Then prune it\n","        self.root = self._prune_tree(X, y, self.root)\n","\n","    def _grow_tree(self, X, y, depth=0):\n","        \"\"\"Recursively grow the decision tree\"\"\"\n","        n_samples, n_features = X.shape\n","        n_labels = len(np.unique(y))\n","\n","        # Stopping criteria\n","        if (depth >= self.max_depth\n","            or n_labels == 1\n","            or n_samples < self.min_samples_split):\n","            leaf_value = self._most_common_label(y)\n","            return J48Node(value=leaf_value)\n","\n","        # Find best split\n","        best_feature, best_threshold, best_splits = self._best_split(X, y)\n","\n","        if best_feature is None:  # No split improves information gain\n","            leaf_value = self._most_common_label(y)\n","            return J48Node(value=leaf_value)\n","\n","        # Split the data\n","        children = {}\n","        if best_threshold is not None:  # Numerical feature\n","            left_idxs = X[:, best_feature] <= best_threshold\n","            right_idxs = X[:, best_feature] > best_threshold\n","            children[\"left\"] = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n","            children[\"right\"] = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n","        else:  # Categorical feature\n","            for value, idxs in best_splits.items():\n","                children[value] = self._grow_tree(X[idxs], y[idxs], depth + 1)\n","\n","        return J48Node(feature=best_feature, threshold=best_threshold, children=children)"]}]}